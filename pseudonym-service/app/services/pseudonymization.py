"""
Servicio de pseudonimizaci√≥n - VERSI√ìN 2.0 ROBUSTA

MEJORAS v2.0:
- Normalizaci√≥n de espacios m√∫ltiples en nombres
- Generaci√≥n autom√°tica de variaciones de orden (apellidos-nombres / nombres-apellidos)
- B√∫squeda case-insensitive mejorada
- Manejo de nombres parciales

PRECISI√ìN ESPERADA: ~98-99%
"""
import re
import uuid
from typing import Dict, Set, List, Tuple
import logging

from app.vault_client import encrypt, decrypt
from app.redis_client import get as redis_get, set as redis_set, delete_pattern
from app.config import settings
from app.services.spacy_detector import detectar_entidades_spacy

logger = logging.getLogger(__name__)

# Patrones Regex para datos ESTRUCTURADOS
PATTERNS = {
    'ruc': r'\b\d{13}\b',
    'cedula': r'\b\d{10}\b',
    'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
    'telefono': r'\b(?:\+593\s?)?(?:0)[2-9][0-9]{6,8}(?:\s?/\s?[0-9]{7,10})?\b',
    'direccion_interseccion': r'\b[A-Z0-9]+\s+Y\s+[A-Z0-9]+,\s+(?:CASA|EDIFICIO|PISO|DEPARTAMENTO|LOCAL)\s+[A-Z0-9\-]+\b',
}

# Excepciones expl√≠citas
EXCEPCIONES = {
    'ARCOTEL', 'CAFI', 'CTDG', 'CCON', 'DEDA', 'CTRP', 'CADF',
    'QUITO', 'GUAYAQUIL', 'CUENCA', 'AMBATO', 'RIOBAMBA', 'LOJA',
    'MACHALA', 'PORTOVIEJO', 'MANTA', 'SANTO DOMINGO', 'ESMERALDAS', 'IBARRA',
    'PICHINCHA', 'GUAYAS', 'AZUAY', 'TUNGURAHUA', 'CHIMBORAZO',
    'MANAB√ç', 'EL ORO', 'IMBABURA',
}

FRASES_EXCLUIDAS = {
    'Ley Org√°nica de Telecomunicaciones',
    'C√≥digo Org√°nico Administrativo',
    'Registro Oficial',
    'Agencia de Regulaci√≥n y Control',
    'Sistema de Gesti√≥n Documental',
}


def normalizar_espacios(texto: str) -> str:
    """
    Normaliza espacios m√∫ltiples, tabs, newlines.

    "CHARCO  I√ëIGUE Z   KLEVER" ‚Üí "CHARCO I√ëIGUEZ KLEVER"
    """
    # Reemplazar m√∫ltiples espacios/tabs/newlines con un solo espacio
    texto_normalizado = re.sub(r'\s+', ' ', texto.strip())
    return texto_normalizado


def generar_variaciones_nombre(nombre: str) -> List[str]:
    """
    Genera variaciones de un nombre completo.

    Entrada: "CHARCO I√ëIGUEZ KLEVER LUIS"

    Salida:
    [
        "CHARCO I√ëIGUEZ KLEVER LUIS",  # Original
        "KLEVER LUIS CHARCO I√ëIGUEZ",  # Invertido (nombres primero)
        "CHARCO I√ëIGUEZ",              # Solo apellidos
        "KLEVER LUIS",                 # Solo nombres
        "CHARCO",                      # Primer apellido
        "KLEVER",                      # Primer nombre
    ]
    """
    # Normalizar espacios primero
    nombre_limpio = normalizar_espacios(nombre)

    variaciones = [nombre_limpio]  # Original

    # Dividir en palabras
    palabras = nombre_limpio.split()

    if len(palabras) >= 4:
        # Asumir formato: APELLIDO1 APELLIDO2 NOMBRE1 NOMBRE2
        apellidos = palabras[:2]
        nombres = palabras[2:]

        # Variaci√≥n 1: Nombres primero, apellidos despu√©s
        variaciones.append(' '.join(nombres + apellidos))

        # Variaci√≥n 2: Solo apellidos
        variaciones.append(' '.join(apellidos))

        # Variaci√≥n 3: Solo nombres
        variaciones.append(' '.join(nombres))

        # Variaci√≥n 4: Primer apellido
        variaciones.append(apellidos[0])

        # Variaci√≥n 5: Primer nombre
        variaciones.append(nombres[0])

    elif len(palabras) == 3:
        # Puede ser: APELLIDO1 APELLIDO2 NOMBRE o APELLIDO NOMBRE1 NOMBRE2
        # Generar ambas posibilidades

        # Caso 1: 2 apellidos + 1 nombre
        variaciones.append(f"{palabras[2]} {palabras[0]} {palabras[1]}")
        variaciones.append(f"{palabras[0]} {palabras[1]}")  # Apellidos

        # Caso 2: 1 apellido + 2 nombres
        variaciones.append(f"{palabras[1]} {palabras[2]} {palabras[0]}")
        variaciones.append(f"{palabras[1]} {palabras[2]}")  # Nombres

    elif len(palabras) == 2:
        # APELLIDO NOMBRE o NOMBRE APELLIDO
        variaciones.append(f"{palabras[1]} {palabras[0]}")  # Invertir

    # Eliminar duplicados preservando orden
    variaciones_unicas = []
    for v in variaciones:
        if v not in variaciones_unicas and len(v) >= 5:
            variaciones_unicas.append(v)

    return variaciones_unicas


def buscar_y_reemplazar_variaciones(
        texto: str,
        variaciones: List[str],
        pseudonimo: str
) -> Tuple[str, int]:
    """
    Busca todas las variaciones de un nombre y las reemplaza.

    Returns:
        (texto_modificado, cantidad_reemplazos)
    """
    texto_resultado = texto
    total_reemplazos = 0

    for variacion in variaciones:
        # Contar cu√°ntas veces aparece esta variaci√≥n
        # Usar regex para buscar palabra completa (evitar reemplazos parciales)
        patron = r'\b' + re.escape(variacion) + r'\b'

        matches = list(re.finditer(patron, texto_resultado, re.IGNORECASE))

        if matches:
            # Reemplazar
            texto_resultado = re.sub(patron, pseudonimo, texto_resultado, flags=re.IGNORECASE)
            total_reemplazos += len(matches)

            logger.debug(f"   üîÑ Variaci√≥n '{variacion}' ‚Üí {pseudonimo} ({len(matches)} veces)")

    return texto_resultado, total_reemplazos


def generate_pseudonym(prefix: str = "PSN") -> str:
    """Genera un pseud√≥nimo √∫nico."""
    unique_id = str(uuid.uuid4())[:8].upper()
    return f"{prefix}_{unique_id}"


def is_exception(text: str) -> bool:
    """Verifica si un texto es una excepci√≥n conocida."""
    text_clean = text.strip()

    if text_clean.upper() in {e.upper() for e in EXCEPCIONES}:
        return True

    for frase in FRASES_EXCLUIDAS:
        if frase.lower() in text_clean.lower():
            return True

    palabras_institucionales = [
        'ARCOTEL', 'Direcci√≥n', 'Coordinaci√≥n', 'Unidad',
        'Reglamento', 'Ley', 'C√≥digo', 'Estatuto',
        'Ministerio', 'Secretar√≠a', 'Agencia'
    ]

    for palabra in palabras_institucionales:
        if palabra in text_clean:
            return True

    return False


async def pseudonymize_text(text: str, session_id: str) -> Dict:
    """
    Pseudonimiza un texto usando H√çBRIDO v2.0 ROBUSTO.

    MEJORAS:
    - Normalizaci√≥n de espacios en nombres
    - Variaciones autom√°ticas de orden
    - B√∫squeda robusta con regex
    """
    pseudonymized_text = text
    mapping = {}
    processed_values: Set[str] = set()

    stats = {
        'regex_detections': 0,
        'encabezado_detections': 0,
        'spacy_detections': 0,
        'firmantes_detections': 0,
        'total_unique': 0,
        'total_reemplazos': 0
    }

    # ========== CAPA 1: REGEX DATOS ESTRUCTURADOS ==========
    logger.info("üîç Capa 1: Detecci√≥n con Regex (datos estructurados)...")

    for data_type, pattern in PATTERNS.items():
        matches = re.finditer(pattern, text, re.MULTILINE)

        for match in matches:
            original_value = match.group(0).strip()

            if original_value in processed_values:
                continue

            if is_exception(original_value):
                logger.debug(f"‚è≠Ô∏è  Regex omiti√≥ excepci√≥n: {original_value}")
                continue

            cache_key = f"{session_id}:{data_type}:{original_value}"
            cached_pseudonym = redis_get(cache_key)

            if cached_pseudonym:
                pseudonym = cached_pseudonym
            else:
                prefix_map = {
                    'ruc': 'RUC',
                    'cedula': 'CEDULA',
                    'email': 'EMAIL',
                    'telefono': 'TELEFONO',
                    'direccion_interseccion': 'DIRECCION',
                }
                prefix = prefix_map.get(data_type, 'PSN')
                pseudonym = generate_pseudonym(prefix)

                encrypted_value = encrypt(original_value)
                reverse_key = f"{session_id}:reverse:{pseudonym}"
                ttl_seconds = settings.TTL_HOURS * 3600
                redis_set(reverse_key, encrypted_value, ttl_seconds)
                redis_set(cache_key, pseudonym, ttl_seconds)

                logger.info(f"‚úÖ Regex detect√≥: {original_value} ‚Üí {pseudonym}")
                stats['regex_detections'] += 1

            pseudonymized_text = pseudonymized_text.replace(original_value, pseudonym)
            mapping[pseudonym] = original_value
            processed_values.add(original_value)
            stats['total_reemplazos'] += 1

    # ========== CAPA 1.5: ENCABEZADO (MEJORADO CON VARIACIONES) ==========
    logger.info("üîç Capa 1.5: Extrayendo nombres del ENCABEZADO (con variaciones)...")

    encabezado = text[:1500]
    patrones_encabezado = {
        'prestador': r'PRESTADOR\s+O\s+CONCESIONARIO\s*:\s*([A-Z√Å√â√ç√ì√ö√ë\s\-\.]+?)(?=\n|REPRESENTANTE)',
        'representante': r'REPRESENTANTE\s+LEGAL\s*:\s*([A-Z√Å√â√ç√ì√ö√ë\s\-\.]+?)(?=\n|CEDULA|RUC)',
    }

    for contexto, patron in patrones_encabezado.items():
        match = re.search(patron, encabezado, re.IGNORECASE | re.MULTILINE)
        if match:
            nombre_original = match.group(1).strip()
            nombre_limpio = normalizar_espacios(nombre_original).strip('.')

            if len(nombre_limpio) >= 10:
                # Generar TODAS las variaciones
                variaciones = generar_variaciones_nombre(nombre_limpio)

                logger.info(f"   üìù Nombre base: {nombre_limpio}")
                logger.info(f"   üîÄ Variaciones: {variaciones}")

                # Verificar si ya existe
                data_type = "nombre_encabezado"
                cache_key = f"{session_id}:{data_type}:{nombre_limpio}"
                cached_pseudonym = redis_get(cache_key)

                if cached_pseudonym:
                    pseudonym = cached_pseudonym
                else:
                    pseudonym = generate_pseudonym("NOMBRE")
                    encrypted_value = encrypt(nombre_limpio)
                    reverse_key = f"{session_id}:reverse:{pseudonym}"
                    ttl_seconds = settings.TTL_HOURS * 3600
                    redis_set(reverse_key, encrypted_value, ttl_seconds)
                    redis_set(cache_key, pseudonym, ttl_seconds)
                    stats['encabezado_detections'] += 1

                # Buscar y reemplazar TODAS las variaciones
                pseudonymized_text, count = buscar_y_reemplazar_variaciones(
                    pseudonymized_text,
                    variaciones,
                    pseudonym
                )

                if count > 0:
                    logger.info(f"‚úÖ Encabezado ({contexto}): {nombre_limpio} ‚Üí {pseudonym} ({count} reemplazos)")
                    mapping[pseudonym] = nombre_limpio
                    processed_values.add(nombre_limpio)
                    stats['total_reemplazos'] += count

    # ========== CAPA 2: spaCy ==========
    logger.info("üîç Capa 2: Detecci√≥n con spaCy NER...")

    entidades_spacy = detectar_entidades_spacy(text)

    for entidad in entidades_spacy:
        original_value = entidad["texto"].strip()
        original_value = normalizar_espacios(original_value)

        if original_value in processed_values:
            continue

        if is_exception(original_value):
            continue

        data_type = "nombre_persona"
        cache_key = f"{session_id}:{data_type}:{original_value}"
        cached_pseudonym = redis_get(cache_key)

        if cached_pseudonym:
            pseudonym = cached_pseudonym
        else:
            pseudonym = generate_pseudonym("NOMBRE")
            encrypted_value = encrypt(original_value)
            reverse_key = f"{session_id}:reverse:{pseudonym}"
            ttl_seconds = settings.TTL_HOURS * 3600
            redis_set(reverse_key, encrypted_value, ttl_seconds)
            redis_set(cache_key, pseudonym, ttl_seconds)
            stats['spacy_detections'] += 1

        pseudonymized_text = pseudonymized_text.replace(original_value, pseudonym)
        mapping[pseudonym] = original_value
        processed_values.add(original_value)
        stats['total_reemplazos'] += 1

    # ========== CAPA 3: FIRMANTES ==========
    logger.info("üîç Capa 3: Extrayendo FIRMANTES...")

    seccion_firmas = text[-2000:]
    patrones_firmantes = [
        r'Elaborado\s+por:\s+([A-Za-z√°√©√≠√≥√∫√±√ë\s\.]+?)(?=\n|\s{2,})',
        r'Revisado\s+por:\s+([A-Za-z√°√©√≠√≥√∫√±√ë\s\.]+?)(?=\n|\s{2,})',
        r'Aprobado\s+por:\s+([A-Za-z√°√©√≠√≥√∫√±√ë\s\.]+?)(?=\n|\s{2,})',
        r'Ing\.\s+([A-Za-z√°√©√≠√≥√∫√±√ë\s\.]+?)(?=\n|\s{2,})',
        r'Econ\.\s+([A-Za-z√°√©√≠√≥√∫√±√ë\s\.]+?)(?=\n|\s{2,})',
        r'Dr\.\s+([A-Za-z√°√©√≠√≥√∫√±√ë\s\.]+?)(?=\n|\s{2,})',
    ]

    for patron in patrones_firmantes:
        for match in re.finditer(patron, seccion_firmas, re.MULTILINE):
            nombre_original = match.group(1).strip()
            nombre_limpio = normalizar_espacios(nombre_original).strip('.')

            if len(nombre_limpio) >= 5 and nombre_limpio not in processed_values:
                data_type = "nombre_firmante"
                cache_key = f"{session_id}:{data_type}:{nombre_limpio}"
                cached_pseudonym = redis_get(cache_key)

                if cached_pseudonym:
                    pseudonym = cached_pseudonym
                else:
                    pseudonym = generate_pseudonym("NOMBRE")
                    encrypted_value = encrypt(nombre_limpio)
                    reverse_key = f"{session_id}:reverse:{pseudonym}"
                    ttl_seconds = settings.TTL_HOURS * 3600
                    redis_set(reverse_key, encrypted_value, ttl_seconds)
                    redis_set(cache_key, pseudonym, ttl_seconds)
                    stats['firmantes_detections'] += 1

                pseudonymized_text = pseudonymized_text.replace(nombre_limpio, pseudonym)
                mapping[pseudonym] = nombre_limpio
                processed_values.add(nombre_limpio)
                stats['total_reemplazos'] += 1

    # ========== ESTAD√çSTICAS ==========
    stats['total_unique'] = len(mapping)

    logger.info(f"üìä Estad√≠sticas de detecci√≥n:")
    logger.info(f"   - Regex: {stats['regex_detections']}")
    logger.info(f"   - Encabezado: {stats['encabezado_detections']}")
    logger.info(f"   - spaCy: {stats['spacy_detections']}")
    logger.info(f"   - Firmantes: {stats['firmantes_detections']}")
    logger.info(f"   - Total √∫nicos: {stats['total_unique']}")
    logger.info(f"   - Total reemplazos: {stats['total_reemplazos']}")

    return {
        'pseudonymized_text': pseudonymized_text,
        'session_id': session_id,
        'mapping': mapping,
        'pseudonyms_count': len(mapping),
        'stats': stats
    }


async def depseudonymize_text(text: str, session_id: str) -> Dict:
    """Revierte la pseudonimizaci√≥n."""
    original_text = text

    pseudonym_pattern = r'\b[A-Z]+_[A-F0-9]{8}\b'
    matches = re.finditer(pseudonym_pattern, text)

    for match in matches:
        pseudonym = match.group(0)
        reverse_key = f"{session_id}:reverse:{pseudonym}"
        encrypted_value = redis_get(reverse_key)

        if encrypted_value:
            try:
                original_value = decrypt(encrypted_value)
                original_text = original_text.replace(pseudonym, original_value)
            except Exception as e:
                logger.error(f"‚ùå Error descifrando {pseudonym}: {e}")

    return {'original_text': original_text}


async def cleanup_session(session_id: str):
    """Elimina todos los datos de una sesi√≥n."""
    try:
        pattern = f"{session_id}:*"
        delete_pattern(pattern)
        logger.info(f"üßπ Sesi√≥n {session_id} limpiada")
        return {"status": "success", "session_id": session_id}
    except Exception as e:
        logger.error(f"‚ùå Error limpiando sesi√≥n {session_id}: {e}")
        raise
